{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fbec56",
   "metadata": {},
   "source": [
    "# Einführung in ML Tutorial\n",
    "\n",
    "In diesem Tutorial werden wir verschiedene Techniken des maschinellen Lernens auf Vibrationsdaten anwenden.\n",
    "Unser Ziel ist es, den Zustand einer Maschine basierend auf Vibrationsdaten zu klassifizieren und spezifische Werte vorherzusagen.\n",
    "\n",
    "**Inhalt:**\n",
    "1. **Feature Engineering**: Vorbereitung der Daten.\n",
    "2. **Supervised Learning (Klassifikation)**: Vorhersage von Kategorien (z. B. 'Normal' vs. 'Hoch').\n",
    "3. **Supervised Learning (Regression)**: Vorhersage kontinuierlicher Werte.\n",
    "4. **Unsupervised Learning**: Mustererkennung ohne Labels.\n",
    "5. **Deep Learning Showcase**: Ein kurzer Einblick in neuronale Netze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Daten laden\n",
    "file_path = 'vibration_messung_2025-10-15_1704-1737.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "print(\"Daten geladen:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaba57a",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Rohdaten sind oft verrauscht. Wir wenden verschiedene Techniken an, um nützliche Informationen zu extrahieren:\n",
    "\n",
    "1.  **Rolling Statistics**: Wir berechnen Mittelwert, Standardabweichung, Min und Max über ein Zeitfenster (Glättung).\n",
    "2.  **Lag Features**: Wir verwenden Werte aus vorherigen Zeitschritten (z. B. t-1), da die Vergangenheit oft die Zukunft beeinflusst.\n",
    "3.  **Rate of Change**: Wir berechnen die Differenz zwischen aufeinanderfolgenden Werten, um Trends zu erfassen.\n",
    "\n",
    "Zusätzlich erstellen wir ein **Label** für das überwachte Lernen: Wenn der Mittelwert über 1000 liegt, klassifizieren wir dies als 'Hohe Vibration' (1), andernfalls als 'Normal' (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne rollierende Statistiken (Fenstergröße 50)\n",
    "window_size = 50\n",
    "df_features = df[['wert']].rolling(window=window_size).agg(['mean', 'std', 'min', 'max']).dropna()\n",
    "df_features.columns = df_features.columns.droplevel(0)\n",
    "\n",
    "# --- NEU: Zusätzliche Features ---\n",
    "# 1. Lag Features (Werte aus dem vorherigen Schritt)\n",
    "df_features['mean_lag1'] = df_features['mean'].shift(1)\n",
    "df_features['std_lag1'] = df_features['std'].shift(1)\n",
    "\n",
    "# 2. Änderungsrate (Differenz zum vorherigen Schritt)\n",
    "df_features['mean_change'] = df_features['mean'].diff()\n",
    "\n",
    "# Entferne NaNs, die durch shift/diff entstanden sind\n",
    "df_features.dropna(inplace=True)\n",
    "# --------------------------------\n",
    "\n",
    "# Erstelle Ziel-Label: \"Hohe\" Vibration (> 1000) vs \"Normal\"\n",
    "# Wir verwenden den Mittelwert des Fensters für das Label\n",
    "threshold = 1000\n",
    "df_features['label'] = (df_features['mean'] > threshold).astype(int)\n",
    "\n",
    "print(\"Features und Labels:\")\n",
    "print(df_features.head())\n",
    "print(f\"Verteilung der Labels:\\n{df_features['label'].value_counts()}\")\n",
    "\n",
    "# --- VISUALISIERUNG ---\n",
    "# 1. Rollierende Statistiken vs. Rohdaten (Erste 1000 Punkte zur Übersichtlichkeit)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df['wert'].iloc[:1000], label='Rohdaten', alpha=0.5)\n",
    "plt.plot(df_features['mean'].iloc[:1000], label='Rollierender Mittelwert (Fenster 50)', linewidth=2)\n",
    "plt.title('1. Rollierende Statistiken: Glättung der Daten')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Lag Features (Autokorrelation)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(df_features['mean'], df_features['mean_lag1'], alpha=0.1, s=1)\n",
    "plt.title('2. Lag Features: Aktueller Mittelwert vs. Vorheriger Mittelwert')\n",
    "plt.xlabel('Mittelwert (t)')\n",
    "plt.ylabel('Mittelwert (t-1)')\n",
    "plt.show()\n",
    "\n",
    "# 3. Änderungsrate\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df_features['mean_change'].iloc[:1000])\n",
    "plt.title('3. Änderungsrate: Stabilität des Signals')\n",
    "plt.ylabel('Änderung im Mittelwert')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca01ff",
   "metadata": {},
   "source": [
    "## Supervised Learning: Klassifikation\n",
    "\n",
    "Hier trainieren wir Modelle, um zu entscheiden, ob ein Zustand kritisch ist oder nicht.\n",
    "Wir vergleichen verschiedene Algorithmen:\n",
    "*   **Decision Tree (Entscheidungsbaum)**: Ein einfaches, interpretierbares Modell, das Daten basierend auf Feature-Werten aufteilt.\n",
    "*   **Random Forest**: Ein Ensemble aus vielen Entscheidungsbäumen. Sehr robust.\n",
    "*   **Support Vector Machine (SVM)**: Versucht, eine optimale Grenze zwischen Klassen zu finden.\n",
    "*   **K-Nearest Neighbors (KNN)**: Klassifiziert basierend auf Ähnlichkeit zu Nachbarn.\n",
    "*   **Gradient Boosting**: Baut Bäume sequenziell auf, um Fehler der vorherigen zu korrigieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Daten vorbereiten\n",
    "X = df_features[['mean', 'std', 'min', 'max', 'mean_lag1', 'std_lag1', 'mean_change']]\n",
    "y = df_features['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Dictionary zum Speichern der Ergebnisse für späteren Vergleich\n",
    "results = {}\n",
    "\n",
    "# --- 1. Decision Tree (Beispiel) ---\n",
    "print(\"Trainiere Decision Tree...\")\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "results['Decision Tree'] = acc_dt\n",
    "print(f\"Decision Tree Genauigkeit: {acc_dt:.4f}\")\n",
    "\n",
    "# --- 2. Random Forest (Beispiel) ---\n",
    "print(\"\\nTrainiere Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=2, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "results['Random Forest'] = acc_rf\n",
    "print(f\"Random Forest Genauigkeit: {acc_rf:.4f}\")\n",
    "\n",
    "# --- 3. Support Vector Machine (TODO) ---\n",
    "# TODO: Initialisiere SVC, trainiere das Modell, mache Vorhersagen und berechne die Genauigkeit\n",
    "print(\"\\nTrainiere SVM (TODO)...\")\n",
    "# svm_model = ...\n",
    "# svm_model.fit(...)\n",
    "# y_pred_svm = ...\n",
    "# acc_svm = ...\n",
    "# results['SVM'] = acc_svm\n",
    "# print(f\"SVM Genauigkeit: {acc_svm:.4f}\")\n",
    "\n",
    "# --- 4. K-Nearest Neighbors (TODO) ---\n",
    "# TODO: Initialisiere KNeighborsClassifier, trainiere, mache Vorhersagen und berechne die Genauigkeit\n",
    "print(\"\\nTrainiere KNN (TODO)...\")\n",
    "# knn_model = ...\n",
    "# ...\n",
    "\n",
    "# --- 5. Gradient Boosting (TODO) ---\n",
    "# TODO: Initialisiere GradientBoostingClassifier, trainiere, mache Vorhersagen und berechne die Genauigkeit\n",
    "print(\"\\nTrainiere Gradient Boosting (TODO)...\")\n",
    "# gb_model = ...\n",
    "# ...\n",
    "\n",
    "# --- Vergleich ---\n",
    "if results:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(results.keys(), results.values(), color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "    plt.title('Vergleich der Klassifikationsmodelle')\n",
    "    plt.ylabel('Genauigkeit')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Noch keine Modelle trainiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea4aed",
   "metadata": {},
   "source": [
    "## Supervised Learning: Regression\n",
    "\n",
    "Bei der Regression wollen wir keinen Zustand (Klasse) vorhersagen, sondern einen **konkreten numerischen Wert**.\n",
    "Aufgabe: Wir versuchen, den **Mittelwert** basierend auf den anderen Features ('std', 'min', 'max') vorherzusagen.\n",
    "\n",
    "Modelle:\n",
    "*   **Linear Regression**: Sucht nach einem linearen Zusammenhang.\n",
    "*   **Decision Tree Regressor**: Ein Entscheidungsbaum für numerische Werte.\n",
    "*   **Random Forest Regressor**: Ein Ensemble von Entscheidungsbäumen für kontinuierliche Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832da069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 1. Daten zuerst bereinigen\n",
    "cols = ['std', 'min', 'max', 'mean', 'mean_lag1', 'std_lag1', 'mean_change']\n",
    "df_clean = df_features[cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "X_reg = df_clean[['std', 'min', 'max', 'mean_lag1', 'std_lag1', 'mean_change']]\n",
    "y_reg = df_clean['mean']\n",
    "\n",
    "# 2. Aufteilen\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Starte Training mit {len(X_train_r)} Beispielen...\")\n",
    "\n",
    "# --- 1. Decision Tree Regressor (Beispiel) ---\n",
    "print(\"\\nTrainiere Decision Tree Regressor...\")\n",
    "dt_reg = DecisionTreeRegressor(random_state=42, max_depth=10)\n",
    "dt_reg.fit(X_train_r, y_train_r)\n",
    "y_pred_dt = dt_reg.predict(X_test_r)\n",
    "\n",
    "mse_dt = mean_squared_error(y_test_r, y_pred_dt)\n",
    "r2_dt = r2_score(y_test_r, y_pred_dt)\n",
    "print(f\"Decision Tree - MSE: {mse_dt:.2f}, R2: {r2_dt:.4f}\")\n",
    "\n",
    "# --- 2. Linear Regression (TODO) ---\n",
    "# TODO: Initialisiere LinearRegression, trainiere, mache Vorhersagen und berechne MSE/R2\n",
    "print(\"\\nTrainiere Linear Regression (TODO)...\")\n",
    "# lr_model = ...\n",
    "# ...\n",
    "# print(f\"Linear Regression - MSE: {mse_lr:.2f}, R2: {r2_lr:.4f}\")\n",
    "\n",
    "# --- 3. Random Forest Regressor (TODO) ---\n",
    "# TODO: Initialisiere RandomForestRegressor, trainiere, mache Vorhersagen und berechne MSE/R2\n",
    "print(\"\\nTrainiere Random Forest Regressor (TODO)...\")\n",
    "# rf_reg = ...\n",
    "# ...\n",
    "# print(f\"Random Forest - MSE: {mse_rf:.2f}, R2: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28511efc",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: K-Means Clustering\n",
    "\n",
    "Hier haben wir keine Labels. Der Algorithmus versucht selbstständig, Gruppen (Cluster) in den Daten zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Trainiere K-Means (wir erwarten 2 Cluster: Normal und Hoch)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df_features.index, df_features['mean'], c=clusters, cmap='viridis', marker='.')\n",
    "plt.title('K-Means Clustering der Vibrationsdaten')\n",
    "plt.xlabel('Zeit')\n",
    "plt.ylabel('Mittlere Vibration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41f60a",
   "metadata": {},
   "source": [
    "## Deep Learning Showcase\n",
    "\n",
    "**Hinweis**: Für strukturierte Daten (Tabellen) wie diese sind klassische ML-Modelle (wie Random Forest oder Gradient Boosting) oft **besser, schneller und einfacher** zu trainieren als Deep Learning.\n",
    "Deep Learning glänzt besonders bei unstrukturierten Daten wie Bildern, Audio oder komplexem Text.\n",
    "\n",
    "Hier zeigen wir dennoch kurz, wie man ein einfaches neuronales Netz (MLP) mit Keras erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (MinMax oft besser für NN)\n",
    "scaler_nn = MinMaxScaler()\n",
    "X_train_nn = scaler_nn.fit_transform(X_train)\n",
    "X_test_nn = scaler_nn.transform(X_test)\n",
    "\n",
    "# Modell erstellen\n",
    "mlp_model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "history_mlp = mlp_model.fit(X_train_nn, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluierung\n",
    "loss, accuracy = mlp_model.evaluate(X_test_nn, y_test)\n",
    "print(f\"MLP Test Genauigkeit: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823c7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
